---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348 Fall 2019"
date: '22 November 2019'
output:
  pdf_document:
    toc: no
  html_document:
    toc: no
    toc_float:
      collapsed: no
      smooth_scroll: yes
---
## Ankur Bhagwath, EID asb3477

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
library(dplyr)
library(MASS)
library(lmtest)
library(sandwich)
library(ggplot2)
library(plotROC)
library(glmnet)

```



**0. (5 pts)** Introduce your dataset and each of your variables (or just your main variables if you have lots) in a paragraph.
```{r}
census <- read.csv('Proj_2_Data.csv')
colnames(census)[1] <- "Age"
head(census, 3)


```
The dataset is a sample of 32,561 people across the country whose census data was taken in 1994. Each person corresponds to one entry. Numeric variables include age, highest grade level completed, capital gains, capital losses, and hours worked per week. Categorical variables include Job type (Public/Private, etc), Education (High school, College, etc), Marital Status, Occupation (field of work), Role in Family (Husband, Wife, etc), Race, Sex, Country of Origin, and Income strata (binomial, less or greater than $50,000)

**1. (15 pts)** 

```{r}
m1 <- manova(cbind(Edunum, Cap_Gain, Cap_Loss, Hours_week) ~ Race, data = census)
summary(m1) 
summary.aov(m1)
pairwise.t.test(census$Edunum, census$Race, p.adj = 'none')
pairwise.t.test(census$Cap_Gain, census$Race, p.adj = 'none')
pairwise.t.test(census$Cap_Loss, census$Race, p.adj = 'none')
pairwise.t.test(census$Hours_week, census$Race, p.adj = 'none')
0.05/45
1-(0.95)^45


```
All the numeric variables were MANOVA tested to see if they showed a mean difference depending on a person's race, and the univariate ANOVA tests found signficant p-values for all numeric variables. The assumptions for MANOVA may not have been met, although sampling is random with few outliers, multivariate normality and linear relationships among dependent variables are unlikely. This is because capital gains and losses are likely to have a skew (most people don't own stock), and education is likely skewed (most people have a high school diploma). Following this, pairwise t-tests were conducted to see how edunum (highest grade reached, 13 for college), Cap_Gain (Capital Gains), Cap_Loss (Capital Losses) and Hours_week (Hours worked per week) differed, on average, based on race, with results shown above. The total number of tests conducted at this point is 1 MANOVA + 4 ANOVA + 40 t tests (10 t-tests per ANOVA) = 45 total. As such, the bonferroni adjusted significance is .05/45 = 0.0011, and probability of a type I error is `r 1 - 0.95^45`. Given this, we can examine significance: for grade level (edunum), all ethnic groups were significantly different from each other except for American Indians/Blacks and American Indians/Other. For Capital Gains, the only differences were between Asian/Black and White/Black. Regarding Capital Losses, only Blacks were significantly different from whites. For hours worked, Asians were different from Blacks, and Blacks were different from Whites. 

**2. (10 pts)** 
```{r}
null_hours <- census$Hours_week 
null_inc <- census$Income %>% as.character()
rando <- vector()
census %>% group_by(Income) %>% summarise(m = mean(Hours_week)) 
45.473 - 38.840
for(i in 1:5000){
  new <- data.frame(inc = null_inc, hours = sample(null_hours))
  rando[i] <- mean(new[new$inc == " >50K",]$hours) - mean(new[new$inc == ' <=50K',]$hours)
   
}
mean(rando > 6.633)*2
hist(rando); abline(v = 6.633) 
ggplot()+geom_histogram(aes(rando)) + geom_vline(xintercept = 6.633)
t.test(data = census, Hours_week~Income)
```
The permutation test performed above was testing to see if there was a true mean difference in hours worked per week between those making a salary greater than $50,000 and those making less than $50,000. The null hypothesis was that there was no mean difference in hours worked per week between the two salary groups, while the alternative hypothesis was that there was a true mean difference in hours worked between the two salary groups. The p-value for this manually conducted permutation test was found to be zero, so we reject the null hypothesis. The true mean difference of hours worked between the two income groups is not zero.  Plots were made to show the null distribution (top graph) with the test statistic (bottom graph). As the bottom graph shows, the test statistic (true experimental mean, 6.633) is very far from the null distribution, hence why p-value was zero. A t-test was also conducted to show that the manually conducted permutation test has similar conclusions to R's t-test.


**3. (35 pts)** 
```{r}
census2 <- census
census2 <- census2 %>% mutate(Edu_centre = Edunum - mean(Edunum))
fit1 <- lm(Hours_week ~ Edu_centre*Race, data = census2)
coeftest(fit1)
#Insert graph here
ggplot(data = census2, aes(Edu_centre, Hours_week, color = Race)) + geom_point(size = .8) + geom_smooth(method = 'lm', se = F)

```
A linear model was made to predict Hours per Week worked from an interaction between highest grade level passed (Edunum centred to Edu_centre) and Race. From this, the coefficient estimates can be interpreted. The intercept suggests that for a Native American with mean education levels, the predicted hours worked is 40.5. Each grade level reached above or below the mean increases hours worked by 0.54 for Native Americans. Compared to Native Americans, the slope (and by extension, the hours worked) is decreased by 0.75 for Asians, decreased by 1.65 for Blacks, by 0.63 for 'Others', and increases by 0.19 for Whites. The interactions show that for Asians, each 1 grade increase in education levels additionally decreases the slope (and so, the number of hours worked) by 0.08, for Blacks it increases the slope by 0.12, for 'Others' it decreases the slope by 0.25, and for Whites it increases the slope by 0.18. Each change in slope corresponds to a change in hours worked. The regression is graphed above. Note that most of the data points correspond to White people, simply because most of the sample is white (data was taken from US residents in 1994), and that vertical lines are formed because education is numeric discrete (grade levels are whole numbers.
```{r}
ggplot(data = census2, aes(x=Edu_centre, y = Hours_week)) + geom_point()
resids1 <- fit1$residuals
fitvals1 <- fit1$fitted.values
ggplot()+geom_point(aes(fitvals1,resids1)) + geom_hline()
bptest(fit1)
ks.test(resids1, 'pnorm', mean = 0, sd(resids1))
coeftest(fit1, vcov = vcovHC(fit1))
#R-Squared Value
(sum((census$Hours_week - mean(census$Hours_week))^2)-sum(fit1$residuals^2))/sum((census$Hours_week-mean(census$Hours_week))^2)

```

The assumptions for the model were checked. Graphs show that neither the linear variables nor their residuals are linear, and the residuals are heteroskedastic (by the Breusch-Pagan test). Additionally, a KS test shows that the normality assumption also fails.
The regression results were recomputed with robust standard errors. There was no change in significance before and after robust standard errors, only the main effect of being Black was signficant in both cases. Generally speaking, all p-values decreased with the application of robust standard errors. As such, the only significant result was that being Black significantly reduces the number of hours worked per week, compared to Native Americans. The proportion of variation in the outcome explained by the outcome (the R-squared value) was 0.024, or 2.4% which means the model is fairly ineffectual in describing correlation.
```{r}
fit2<-lm(Hours_week ~ Edu_centre + Race, data = census2)
coeftest(fit2)
anova(fit1, fit2, test = 'LRT')
```

Without any interaction, the highest grade level reached (Edu_centre, the centered version) is significant, which it was not with the interaction. Being black is still a significant factor on the number of hours worked per week, both with and without the interaction. Conducting a likelihood ratio test, it can be seen that the p-value is >0.05, so the smaller model is better than the larger model.

**4. (5 pts)** 
```{r}
#Bootstrapped standard errors
samp_distn<-replicate(1000, {
  boot_dat<-census2[sample(nrow(census2),replace = TRUE),]
  fit<-lm(Hours_week~Edu_centre*Race,data= boot_dat) 
  coef(fit)
})
samp_distn%>%t%>%as.data.frame%>%summarise_all(sd)

```
The bootstrapped standard errors are shown in the table above. The bootstrapped SEs are all somewhat smaller than the original SEs, as a result, the p-values for the regression would be smaller if the bootstrapped SEs were used, since the test statistic would be more standard errors away from the mean (see original and robust SEs in part 3). Comparing the bootstrapped SEs to the robust SEs, we find that while some of the bootstrapped SEs are smaller than the robust SEs, others are larger. As such, it cannot be absolutely determined as to which of the two methods produces smaller p-values as a whole. Generally speaking, if the SE is larger, than the p-value will be larger, because the test statistic will be fewer SEs away from the mean as compared to if the SE is smaller. 

**5. (40 pts)** 
```{r}
census2 <- census2%>%mutate(Inc_b = case_when(Income == ' <=50K' ~ 0, Income == ' >50K' ~ 1))
fit3 <- glm(Inc_b ~ Marital * Sex, data = census2, family = binomial(link = 'logit'))
coeftest(fit3)
exp(coef(fit3)) %>% round(2) 

```
The binary categorical variable was income, with value of 1 if >50,000 USD, and a value of 0 if <50,000 USD. The coefficient estimates are seen above, and to interpret them, the coefficients were exponentiated. The intercept (exponentiated) shows that a divorced female has odds of 0.07 for having an income above $50,000. The remaining main effect exponentiated coefficients change the odds; to find the odds of having an income above 50K for the other categories, we multiply the exponentiated intercept by the exponentiated coefficients. So, the odds for having above 50K for females are `r .072*11.63` for married couples, `r .072 * .79` if spouse is absent, `r .072*.5` if never married, `r .072*.39` if separated, `r .072*.82` if widowed. As such, we can see that married women are most likely to have higher incomes, whereas separated women have the worst odds for higher incomes, followed by never married women. 
Being male and divorced gives odds of `r .072 * 2.66`, much higher than for women. The interactions odds for men in different marital situations can be interpreted as the odds of having above 50K income in men of a certain marital status being some factor different from the odds for men who are divorced. As an example calculation, For widowed men, the odds are 0.07(Intercept) * 2.66(Male) * 0.82(Widowed) * 1.92(Interaction), which is `r .072*2.66*.82*1.92`. For men, the odds of having above 50K in income are `r 0.072*2.66*.36*11.63` if married, `r 0.072*2.66*.79*.8` if spouse absent, `r 0.072*2.66*.5*.61` if never married, and `r 0.072*2.66*.39*1.93` if separated. From this, we may conclude that married men have the best odds of having an income greater than $50,000, and men who never marry have the worst odds of having incomes greater than 50,000. This is similar to the results for women, so the evidence would suggest either that marriage increases the odds of higher income jobs, or that those with higher income jobs are more likely to be married. 
```{r}
census2$prob1 <- predict(fit3, type = 'response')
table(predictions_0.5 = ifelse(census2$prob1>0.5,1,0), truth = census2$Inc_b) %>% addmargins
table(predictions_0.4 = ifelse(census2$prob1>0.4,1,0), truth = census2$Inc_b) %>% addmargins
census2$logit<-predict(fit3) #get predicted log-odds
ggplot(census2,aes(logit, fill=Income))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)
rplot1 <- ggplot(census2)+geom_roc(aes(d=Inc_b, m = prob1), n.cuts = 0)
rplot1
calc_auc(rplot1)


```
Next, a confusion matrix was made, as shown above. As the table shows, the model is clearly not very good, since it failed to predict a single true value at all with a cutoff value of 0.5. It can, however, predict true values at lower cutoffs, although the criterion is more lenient. Using the 0.5 cutoff value, the Sensitivity is 0 (no true positives), Specificity is 1 (only predicted negatives), recall is 0 (none classified positive), and accuracy is `r 24720/32561`. If cutoff is 0.4, sensitivity is 6702/7841 = `r 6702/7841`, specificity is 16423/24720 = `r 16423/24720`, recall is 6702/14999 = `r 6702/14999` and accuracy is (16423+6702)/32561 = `r (16423+6702)/32561`. Then, we plotted the density of log-odds by outcome, as well as the ROC curve. The log-odds density was all negative, so only false negatives were present, no false positives (which matches the confusion matrix). The AUC was 0.775, which is not very good. An ROC curve plots sensitivity against specificity, ideally, the curve should be extremely steep at low values of FPR (false positive), all the way up to a value near 1 for TPR (true positive), and level out from there, this would signify an ideal model. Since AUC is area under the ROC curve, the ideal AUC would be nearly 1 in this case. However, a very bad model would simple have a diagnol line with a slope of 1, and the area under this curve (AUC) would be 0.5. Since this AUC falls right in the middle of 0.5 and 1, with AUC = 0.775, we may conclude that the model has mediocre predictive power, but it could be worse. Specifically, 0.775 is the probability that a randomly selected person with income >50K has a higher prediction than a random person with under 50K income.
```{r}
class_diag<-function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  data.frame(acc,sens,spec,ppv,auc)
}
k=10
data1<-census2[sample(nrow(census2)),]
folds<-cut(seq(1:nrow(census2)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
truth<-test$Inc_b
fit4<-glm(Inc_b~Marital*Sex,data=train,family="binomial")
probs<-predict(fit4,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)

```

Finally, a 10-fold CV was done. Accuracy was 0.759, Sensitivity 0.000, and Specificity was 0.999. Recall (PPV) was an NA value, likely because the model did not classify anything as positive, although it should technically be 0.  

**6. (10 pts)**

```{r}
census2 <- census2 %>% dplyr::select(-c(logit, prob1, Country, Inc_b, ID, Edunum))
#census2 <- census2 %>% dplyr::select(-Inc_b)
fit5 <- glm(Income ~ -1+., data = census2, family = binomial('logit'))
m1 <- model.matrix(fit5)
m1 <- scale(m1)
m2<- as.matrix(census2$Income)
c1 <- cv.glmnet(m1,m2, family = 'binomial')
l1<-glmnet(m1,m2, family = 'binomial', lambda = c1$lambda.1se)
coef(l1)
```
From the LASSO, it appears that Age, certain types of occupation (too long to write out, see LASSO above), having an associates or professional degree, one's highest grade level in school, being married, never being married, relationship status in the family, being white, being male, one's Capital Gains and Losses, and hours worked per week are all significant variables. Again, full details are listed in the LASSO above. Now, only these significant variables were plugged into a 10-fold CV, seen below.
```{r}
census3 <- census2
census3 <- census3 %>% mutate(n_job = ifelse(Job != ' Local-gov' & Job != ' Never-worked' & Job != ' ?', 1,0 ))
census3 <- census3 %>% mutate(n_edu = ifelse(Education == ' Assoc-acdm' | Education == ' Prof-school', 1,0 ))
census3 <- census3 %>% mutate(n_mar = ifelse(Marital != ' Married-spouse-absent' & Marital != ' Separated' & Marital != ' Widowed', 1,0 ))
census3 <- census3 %>% mutate(n_occ = ifelse(Occupation != ' Adm-clerical' & Occupation != ' Armed Forces' & Occupation != ' Craft-repair', 1,0 ))
census3 <- census3 %>% mutate(n_rel = ifelse(Relation != ' Not-in-family' & Relation != ' Unmarried', 1,0 ))
census3 <- census3 %>% mutate(n_race = case_when(Race == ' White' | Race == ' Other' ~ 1, Race != ' White' ~ 0))
k=10
data1<-census3[sample(nrow(census3)),]
folds<-cut(seq(1:nrow(census3)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
truth<-test$Income
fit6<-glm(Income~Age+n_job+n_edu+n_mar+n_occ+n_rel+n_race+Edu_centre+Hours_week+Cap_Gain+Cap_Loss+Sex,data=train, family = 'binomial')
probs<-predict(fit6,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)



```

The 10-fold CV was run, with accuracy of 0.835, sensitivity at 0.51, specificity at 0.938, PPV at 0.72 and AUC as 0.877 (a good value) as shown above in the output. The AUC was a fairly good value (0.877), as was the accuracy (0.835), and both were significant improvements over the model made in part 5, which had poor AUC of 0.774 and accuracy of 0.759. We may therefore conclude that the LASSO made our model more robust.